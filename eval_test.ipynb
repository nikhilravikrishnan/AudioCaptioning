{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Script Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current sys.path settings:\n",
      "['/home/christian/GaTech/DL/MusicCaptioning', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '/home/christian/GaTech/DL/MusicCaptioning/venv/lib/python3.8/site-packages', '/home/christian/GaTech/DL/MusicCaptioning', '/home/christian/GaTech/DL/MusicCaptioning/models']\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using device cpu for model evaluation.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 238, in <module>\n",
      "    main()\n",
      "  File \"main.py\", line 226, in main\n",
      "    metrics = evaluate(model, mode=\"eval\")\n",
      "  File \"main.py\", line 158, in evaluate\n",
      "    metrics[\"mrr\"] = eval_model_embeddings(model, dataloader, \"MRR\")\n",
      "  File \"/home/christian/GaTech/DL/MusicCaptioning/util/utils.py\", line 28, in eval_model_embeddings\n",
      "    print(len(dataLoader))\n",
      "  File \"/home/christian/GaTech/DL/MusicCaptioning/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/christian/GaTech/DL/MusicCaptioning/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 671, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/christian/GaTech/DL/MusicCaptioning/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/christian/GaTech/DL/MusicCaptioning/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"/home/christian/GaTech/DL/MusicCaptioning/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/home/christian/GaTech/DL/MusicCaptioning/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"/home/christian/GaTech/DL/MusicCaptioning/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"/home/christian/GaTech/DL/MusicCaptioning/venv/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 163, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "RuntimeError: stack expects each tensor to be equal size, but got [1, 30] at entry 0 and [1, 35] at entry 11\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py --config \"/home/christian/GaTech/DL/MusicCaptioning/configs/resnet_eval.yaml\" --mode=\"eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4680f382825ea977b84f25ecbf33b78e5963db3eebe4fffb9a6d3d97e2e0a765"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
