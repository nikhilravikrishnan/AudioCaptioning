{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4850770f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T01:41:12.585027Z",
     "start_time": "2022-11-06T01:41:09.583978Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchlibrosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m RobertaTokenizer, ViTFeatureExtractor, ViTModel\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclip\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseClip\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataloaders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclotho_dataloader\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioCaptioningDataset\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n",
      "File \u001b[1;32mc:\\Users\\wabec\\MSA\\7643\\Project\\MusicCaptioning\\models\\clip.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtext_encoder\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39maudio_encoders\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m resnet50, ResNet50_Weights\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m ViTModel, ViTFeatureExtractor\n",
      "File \u001b[1;32m~\\MSA\\7643\\Project\\MusicCaptioning\\models\\audio_encoders.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m \u001b[39mimport\u001b[39;00m create_feature_extractor\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchlibrosa\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstft\u001b[39;00m \u001b[39mimport\u001b[39;00m Spectrogram, LogmelFilterBank\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchlibrosa\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maugmentation\u001b[39;00m \u001b[39mimport\u001b[39;00m SpecAugmentation\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_audio_feature_vector\u001b[39m(model: nn\u001b[39m.\u001b[39mModule, img_tensors: torch\u001b[39m.\u001b[39mTensor, layer_dict\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mavgpool\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m}):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchlibrosa'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+ '/code')\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer, ViTFeatureExtractor, ViTModel\n",
    "from models.clip import BaseClip\n",
    "from dataloaders.clotho_dataloader import AudioCaptioningDataset\n",
    "from util import utils\n",
    "import yaml\n",
    "import argparse\n",
    "import torch.utils.data\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from dataloaders.clotho_dataloader import AudioCaptioningDataset\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce35c6a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T01:41:26.552686Z",
     "start_time": "2022-11-06T01:41:24.859888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e855b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-06T01:41:28.780418Z",
     "start_time": "2022-11-06T01:41:28.771036Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = AudioCaptioningDataset(data_dir = 'C:/Users/wabec/MSA/7643/Project/dataset/', split='eva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a5cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.load(os.path.join('C:/Users/wabec/MSA/7643/Project/dataset/clotho_dataset_dev', 'clotho_file_Wooden Door 01.wav_4.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d5a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = x.features[0]\n",
    "caption = x.caption[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6520632d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SOS> The noise is squeaking and banging over and over again. <EOS>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1adc6b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The noise is squeaking and banging over and over again.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the SOS and EOS tokens  \n",
    "caption[6:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a4ae12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.33204174, -10.24599171, -10.37603664, ..., -10.63476944,\n",
       "        -10.66018677, -10.61836052],\n",
       "       [-11.65538502, -11.29440689, -11.86908627, ..., -10.19854069,\n",
       "        -10.22593689, -10.09584808],\n",
       "       [ -7.98115396,  -9.13621426, -11.04828072, ..., -10.18153667,\n",
       "         -9.95860863, -10.09787655],\n",
       "       ...,\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = np.zeros(((40*44100+1)//512, 64) )\n",
    "spec[:spectrogram.shape[0], :spectrogram.shape[1]] = spectrogram\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c47182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3445, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7299eb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3445, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "spec = torch.from_numpy(spec)\n",
    "torch.stack([spec,spec,spec], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517c1168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wabec\\anaconda3\\envs\\7643\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.clotho_dataloader import AudioCaptioningDataset\n",
    "\n",
    "train_dataloader = AudioCaptioningDataset(data_dir = 'C:/Users/wabec/MSA/7643/Project/dataset/', \n",
    "                                            split='train/val',\n",
    "                                            vocab_file = 'C:/Users/wabec/MSA/7643/Project/vocab/words_list.p', \n",
    "                                            audio_encoder = 'ResNet50', \n",
    "                                            layer_dict={\"avgpool\":\"features\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6aea79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataloaders.clotho_dataloader.AudioCaptioningDataset at 0x1b3eb00bbb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = AudioCaptioningDataset(data_dir = 'C:/Users/wabec/MSA/7643/Project/dataset/', \n",
    "                                            split='test',\n",
    "                                            vocab_file = 'C:/Users/wabec/MSA/7643/Project/vocab/words_list.p', \n",
    "                                            audio_encoder = 'ResNet50', \n",
    "                                            layer_dict={\"avgpool\":\"features\"})"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.8.13 ('7643')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "770e5862d7fc8345bc8a92989440beeffd69d0e92ceebfbf53394a7ecdae2263"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
